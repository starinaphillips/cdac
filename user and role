âœ… DEVOPS LAB â€“ PART-1

IAM User & Role Management (STEP-BY-STEP REFERENCE) ğŸ¯ Objective (What Part-1 Achieves)

Implement proper AWS IAM access control using:

    Users (humans)

    Groups (teams)

    Roles (AWS services)

    Policies (permissions)

Verification via EC2 + AWS CLI
ğŸ”¹ STEP 1 â€” Create IAM User (Human Identity)

Path:
AWS Console â†’ IAM â†’ Users â†’ Create user

Configuration

User name: developer

Access type:

âœ… AWS Management Console access

âŒ Programmatic access (unchecked)

Password: Auto-generate

âœ… Require password reset on first login

âŒ Do NOT attach permissions yet

Outcome

IAM user created

Represents a human developer
ğŸ”¹ STEP 2 â€” Create IAM Group (Team Identity)

Path:
IAM â†’ User groups â†’ Create group

Configuration

Group name: DevTeam

Permissions: None (for now)

Outcome

Group created to manage shared permissions

ğŸ”¹ STEP 3 â€” Add User to Group

Path:
IAM â†’ User groups â†’ DevTeam â†’ Add users

Action

Add user: developer

Outcome

User inherits permissions assigned to the group

ğŸ”¹ STEP 4 â€” Attach EC2 Full Access to Group

Path:
IAM â†’ User groups â†’ DevTeam â†’ Permissions â†’ Add permissions

Policy attached

AmazonEC2FullAccess

Why

All developers need EC2 access

Group-level permission = scalable & clean

Outcome

Any user in DevTeam can manage EC2

ğŸ”¹ STEP 5 â€” Attach S3 Read-Only Access to User

Path:
IAM â†’ Users â†’ developer â†’ Permissions â†’ Add permissions

Policy attached (directly to user)

AmazonS3ReadOnlyAccess

Why

Demonstrates fine-grained control

Not all users should have same S3 access

Outcome

User can read S3 but cannot modify objects

ğŸ”¹ STEP 6 â€” Create IAM Role for EC2 (Service Identity)

Path:
IAM â†’ Roles â†’ Create role

    Trusted entity

    AWS service

    Use case: EC2

    Permissions

Attach policy: AmazonS3ReadOnlyAccess

Role name

EC2-S3-ReadRole

Trust relationship (must exist)

{
  "Service": "ec2.amazonaws.com"
}

Outcome

EC2 instances can access S3 securely without keys

ğŸ”¹ STEP 7 â€” Launch EC2 Instance (Verification Instance)

Path:
EC2 â†’ Instances â†’ Launch instance

Configuration

AMI: Amazon Linux

Instance type: t2.micro / t3.micro

Key pair: create new (devops-lab-key)

Public IP: Enabled

Security group:

SSH (22) â†’ 0.0.0.0/0 (temporary for lab)

Outcome

EC2 instance running and reachable

ğŸ”¹ STEP 8 â€” Attach IAM Role to EC2

Path:
EC2 â†’ Instances â†’ Select instance
Actions â†’ Security â†’ Modify IAM role

Attach

EC2-S3-ReadRole

Outcome

EC2 now assumes IAM role automatically

ğŸ”¹ STEP 9 â€” Connect to EC2 (SSH)

From local system

chmod 400 devops-lab-key.pem
ssh -i devops-lab-key.pem ec2-user@<public-dns>

Expected prompt

ec2-user@ip-172-31-x-x

ğŸ”¹ STEP 10 â€” Verify Role-Based Access (MOST IMPORTANT)

Identity verification

aws sts get-caller-identity

Expected

assumed-role/EC2-S3-ReadRole

âœ” Confirms EC2 is using IAM role âœ” No access keys involved

Optional S3 verification

aws s3 ls

âœ” Works without aws configure âœ” Confirms role permissions
ğŸ”¹ STEP 11 â€” Security Validation

ls ~/.aws

Expected

No such file or directory

âœ” No stored credentials âœ” Pure role-based access (best practice)

âœ… PART-1 FINAL STATE (REFERENCE CHECKLIST) Item Status: IAM user created âœ…

IAM group created âœ…

User added to group âœ…

EC2 full access via group âœ…

S3 read-only via user âœ…

IAM role for EC2 created âœ…

Role attached to EC2 âœ…

Verified via STS âœ…
âœ… DEVOPS LAB â€” PART-2

EC2 Deployment & Web Server Hosting

Status: COMPLETED & VERIFIED

ğŸ¯ Objective (What Part-2 Achieves)

Deploy a publicly accessible web server on AWS using:

    EC2 (Amazon Linux)

    Apache HTTP Server

    Proper security group configuration

    Web content deployment

    Browser-level verification

ğŸ”¹ STEP-BY-STEP REFERENCE (PART-2)
ğŸ”¹ STEP 1 â€” Launch EC2 Instance (Web Server)

Path:
AWS Console â†’ EC2 â†’ Instances â†’ Launch instance

Configuration

Name: acts-training-web

AMI: Amazon Linux (Amazon Linux 2023)

Instance type: t2.micro / t3.micro

Key pair: devops-lab-key

Public IP: Enabled

Outcome

EC2 instance created and reachable from the internet

ğŸ”¹ STEP 2 â€” Configure Network & Security Group

Security Group (newly created)

Inbound Rules

Type	Port	Source
SSH	22	My IP / 0.0.0.0/0
HTTP	80	0.0.0.0/0

Notes

    SSH required for admin access

    HTTP required for web access

    HTTPS not required for this lab

Outcome

Instance accessible via SSH and browser

ğŸ”¹ STEP 3 â€” Add Mandatory Tags

Path:

Launch Instance â†’ Resource tags â†’ Add tag

Key Value

Project	ACTS-Training
Owner	Admin

Outcome

Instance properly tagged for identification and evaluation

ğŸ”¹ STEP 4 â€” Launch Instance & Health Check

Instance state: Running

Status checks: 2/2 passed

Outcome

Instance ready for configuration

ğŸ”¹ STEP 5 â€” Connect to EC2 via SSH

From local system

chmod 400 devops-lab-key.pem
ssh -i devops-lab-key.pem ec2-user@<public-dns>

Notes

    ec2-user is mandatory for Amazon Linux

    SSH key permissions enforced

Outcome

Secure shell access established

ğŸ”¹ STEP 6 â€” Install & Start Apache Web Server

sudo yum update -y
sudo yum install httpd -y
sudo systemctl start httpd
sudo systemctl enable httpd

Verification

systemctl status httpd

Expected:

Active: active (running)

Outcome

Apache installed, running, and persistent across reboots

ğŸ”¹ STEP 7 â€” Deploy Web Content (HTML Page)

Navigate to Apache web root

cd /var/www/html

Create HTML file

sudo nano index.html

HTML content

<!DOCTYPE html>
<html>
<head>
  <title>ACTS Training Portal</title>
</head>
<body>
  <h1>Welcome to ACTS Training Portal</h1>
  <p>EC2 Web Server successfully deployed.</p>
</body>
</html>

Fix permissions

sudo chmod 644 index.html

Outcome

Web page created and readable by Apache

ğŸ”¹ STEP 8 â€” Browser Verification (Final Proof)

Open browser and visit

http://<EC2-Public-IP or Public-DNS>

Expected Output

Welcome to ACTS Training Portal
EC2 Web Server successfully deployed.

Outcome

Successful end-to-end validation

âœ… PART-2 FINAL CHECKLIST Requirement Status

EC2 instance launched âœ…

Security group (SSH + HTTP) âœ…

Apache installed & running âœ…

HTML page deployed âœ…

Tags added âœ…

Webpage accessible via browser âœ…
âœ… DEVOPS LAB â€” PART-3

AWS Lambda Basics & Logging

Status: COMPLETED & VERIFIED

ğŸ¯ Objective (What Part-3 Achieves)

Understand and demonstrate:

    Serverless execution using AWS Lambda

    Python-based Lambda functions

    Function testing

    Logging using Amazon CloudWatch

    Log groups and log streams

This part proves you understand how code runs without servers.

ğŸ”¹ STEP-BY-STEP REFERENCE (PART-3)
ğŸ”¹ STEP 1 â€” Open AWS Lambda Service

Path:
AWS Console â†’ Search â†’ Lambda â†’ Create function

ğŸ”¹ STEP 2 â€” Create Lambda Function

Choose:

Author from scratch

Fill in details:

Field	Value
Function name	basic-hello-lambda
Runtime	Python 3.9 / 3.10
Architecture	x86_64
Permissions	Create a new role with basic Lambda permissions

ğŸ‘‰ Click Create function

Outcome

Lambda function created

IAM execution role auto-created with CloudWatch access
ğŸ”¹ STEP 3 â€” Write Basic Lambda Code

In the Code source editor, replace existing code with:

def lambda_handler(event, context):
    return "Hello from AWS Lambda"

Click Deploy
ğŸ”¹ STEP 4 â€” Test the Lambda Function

Click Test

Configure test event:

Event name: test-event

Template: Hello World

Save

Click Test again

Expected Output

Hello from AWS Lambda

Outcome

Lambda executed successfully

Serverless execution verified

ğŸ”¹ STEP 5 â€” Modify Lambda for Logging

Update the code to demonstrate logging:

import datetime

def lambda_handler(event, context):
    print("Current time:", datetime.datetime.now())
    print("Lambda function name:", context.function_name)
    return "Logging successful"

Click Deploy
ğŸ”¹ STEP 6 â€” Test Logging

Click Test

Expected Output
Logging successful

This confirms:

    Code executed

    Logs generated

ğŸ”¹ STEP 7 â€” View Logs in CloudWatch

Path:

Lambda â†’ basic-hello-lambda â†’ Monitor â†’ View logs in CloudWatch

You will see:

âœ” Log Group /aws/lambda/basic-hello-lambda

âœ” Log Streams

Examples:

2025/12/23/[LATEST]xxxxxxxx

Each log stream = one Lambda invocation.
ğŸ”¹ STEP 8 â€” Verify Log Contents

Open the latest log stream and verify:

Current time: <timestamp>
Lambda function name: basic-hello-lambda

âœ… PART-3 FINAL CHECKLIST

Requirement Status

Lambda function created âœ…

Python runtime used âœ…

Function tested âœ…

Output returned âœ…

CloudWatch log group created âœ…

Log streams visible âœ…

Execution details logged âœ…
âœ… DEVOPS LAB â€” SECTION-5

Lambda to Rename Uploaded Files in S3

Status: COMPLETED & VERIFIED

ğŸ¯ Objective

Automatically rename any file uploaded to an S3 bucket by:

    Adding the prefix processed_

    Storing the renamed file in the same bucket

    Achieving this using AWS Lambda + S3 event triggers

ğŸ§  Architecture Overview

S3 Upload
   â†“
S3 ObjectCreated Event
   â†“
AWS Lambda Function
   â†“
Copy Object (new name)
   â†“
Delete Original Object

ğŸ”¹ STEP-BY-STEP REFERENCE (SECTION-5)
ğŸ”¹ STEP 5.1 â€” Create S3 Bucket

Path:
AWS Console â†’ S3 â†’ Create bucket

Configuration

Bucket name: lambda-rename-demo-<unique>

Region: Same as Lambda

Block public access: Enabled

Outcome

Private S3 bucket ready for event triggers
ğŸ”¹ STEP 5.2 â€” Create Lambda Function

Path:
AWS Console â†’ Lambda â†’ Create function â†’ Author from scratch

Configuration

Field	Value
Function name	s3-rename-lambda
Runtime	Python 3.9 / 3.10
Permissions	Create new role with basic Lambda permissions

Outcome

Lambda function created

IAM execution role auto-created

ğŸ”¹ STEP 5.3 â€” Grant S3 Permissions to Lambda

Path:
Lambda â†’ Configuration â†’ Permissions â†’ Execution role â†’ IAM

    Policy attached

    AmazonS3FullAccess

    Reason

    Lambda must copy and delete objects in S3

(Least-privilege policies are implemented later in Section-7)
ğŸ”¹ STEP 5.4 â€” Implement Rename Logic (Lambda Code)

Lambda â†’ Code source

import urllib.parse
import boto3

s3 = boto3.client('s3')

def lambda_handler(event, context):
    record = event['Records'][0]
    bucket = record['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(record['s3']['object']['key'])

    # Prevent infinite loop
    if key.startswith("processed_"):
        return {"statusCode": 200, "body": "Already processed"}

    new_key = "processed_" + key

    s3.copy_object(
        Bucket=bucket,
        CopySource={'Bucket': bucket, 'Key': key},
        Key=new_key
    )

    s3.delete_object(Bucket=bucket, Key=key)

    return {
        "statusCode": 200,
        "body": f"Renamed {key} to {new_key}"
    }

Click Deploy
ğŸ”¹ STEP 5.5 â€” Configure S3 Trigger

Method used (stable approach):
S3 â†’ Bucket â†’ Properties â†’ Event notifications

Event Notification Configuration

Field	Value
Event name	rename-trigger
Event type	All object create events
Prefix	(empty)
Suffix	(empty)
Destination	Lambda function
Lambda	s3-rename-lambda

Outcome

S3 automatically invokes Lambda on every upload
ğŸ”¹ STEP 5.6 â€” Verification (Proof of Completion)

Action

Upload a file: test.txt

Expected Result

test.txt â†’ âŒ removed

processed_test.txt â†’ âœ… created automatically

Optional

CloudWatch logs show successful invocation

âœ… SECTION-5 FINAL CHECKLIST

Requirement Status

S3 bucket created âœ…

Lambda function created âœ…

S3 permissions granted âœ…

Rename logic implemented âœ…

S3 trigger configured âœ…

File renamed automatically âœ…

âœ… DEVOPS LAB â€” SECTION-6 Lambda to Move Files Between Folders in S3

Status: COMPLETED & VERIFIED

ğŸ¯ Objective

Automatically move files in an S3 bucket such that:

    Files uploaded to incoming/ are moved to processed/

    Movement is handled using AWS Lambda

    No manual intervention is required

ğŸ§  Core Concept (Must Remember)

S3 has no real folders

â€œFoldersâ€ are key prefixes

Move operation = Copy + Delete

Lambda is triggered using prefix-based S3 events

ğŸ— Architecture Overview

Upload file to incoming/
        â†“
S3 ObjectCreated event (prefix: incoming/)
        â†“
AWS Lambda function
        â†“
Copy object to processed/
        â†“
Delete object from incoming/

ğŸ”¹ STEP-BY-STEP REFERENCE (SECTION-6)
ğŸ”¹ STEP 6.1 â€” Prepare Folder Structure in S3

Path:
AWS Console â†’ S3 â†’ Bucket (same bucket as Section-5)

Create folders:

incoming/
processed/

These are logical prefixes, not actual directories.
ğŸ”¹ STEP 6.2 â€” Create Lambda Function

Path:
AWS Console â†’ Lambda â†’ Create function â†’ Author from scratch

Configuration

Field	Value
Function name	s3-move-lambda
Runtime	Python 3.9 / 3.10
Permissions	Create new role with basic Lambda permissions

ğŸ”¹ STEP 6.3 â€” Grant S3 Permissions to Lambda

Path:
Lambda â†’ Configuration â†’ Permissions â†’ Execution role â†’ IAM

Policy attached

AmazonS3FullAccess

Reason

Lambda needs permission to copy and delete S3 objects

(Least-privilege IAM policies are implemented later in Section-7)
ğŸ”¹ STEP 6.4 â€” Implement Move Logic (Lambda Code)

Lambda â†’ Code tab â†’ lambda_function.py

import urllib.parse
import boto3

s3 = boto3.client('s3')

def lambda_handler(event, context):
    record = event['Records'][0]
    bucket = record['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(record['s3']['object']['key'])

    # Process only incoming/ files
    if not key.startswith("incoming/"):
        return {"statusCode": 200, "body": "Not an incoming file"}

    filename = key.split("/")[-1]
    new_key = f"processed/{filename}"

    s3.copy_object(
        Bucket=bucket,
        CopySource={'Bucket': bucket, 'Key': key},
        Key=new_key
    )

    s3.delete_object(Bucket=bucket, Key=key)

    return {
        "statusCode": 200,
        "body": f"Moved {key} to {new_key}"
    }

Click Deploy.
ğŸ”¹ STEP 6.5 â€” Configure S3 Trigger (Prefix-Based)

Preferred stable method: Configure from S3 side

Path:
S3 â†’ Bucket â†’ Properties â†’ Event notifications â†’ Create event notification

Configuration

Field	Value
Event name	incoming-move-trigger
Event type	All object create events
Prefix	incoming/
Suffix	(empty)
Destination	Lambda
Lambda function	s3-move-lambda

Save changes.
ğŸ”¹ STEP 6.6 â€” Verification (Proof of Completion)

Action Upload a file:

incoming/testfile.txt

Expected Result

incoming/testfile.txt âŒ removed

processed/testfile.txt âœ… appears automatically

Optional:

CloudWatch logs show successful Lambda invocation

âœ… SECTION-6 FINAL CHECKLIST Requirement Status

incoming/ prefix exists âœ…

processed/ prefix exists âœ…

Lambda function created âœ…

S3 permissions granted âœ…

Prefix-based trigger configured âœ…

File moved automatically âœ…
âœ… DEVOPS LAB â€” SECTION-7

S3 Bucket Access Isolation Using IAM Roles (EC2)

Status: COMPLETED & VERIFIED

ğŸ¯ Objective (What This Section Proves)

    To demonstrate strict access isolation using IAM Roles such that:

    Two separate S3 buckets exist

    Two separate IAM roles exist

    Two separate EC2 instances exist

Each EC2 instance:

    Can access only its assigned S3 bucket

    Is explicitly denied access to the other bucket

    No IAM users

    No access keys

    Access is verified from inside EC2 using AWS CLI

This proves understanding of:

    Least privilege

    Role-based access

    Explicit deny precedence

    Secure cloud design

ğŸ§  Core Concepts (DO NOT SKIP)

1ï¸âƒ£ IAM Role vs IAM User

IAM User â†’ long-term credentials (NOT used here)

IAM Role â†’ temporary credentials (BEST PRACTICE for EC2)

2ï¸âƒ£ Trust Policy vs Permission Policy

    Policy Type Purpose
    Trust policy Who can assume the role
    Permission policy What the role can do

These must never be mixed.

3ï¸âƒ£ Explicit Deny Rule

    Deny always overrides Allow

    Used to enforce strict isolation

    Mandatory in this section

ğŸ— Architecture Overview

EC2-A â”€â”€(TrainingBucketRole)â”€â”€â–º training-bucket-a
        âŒ project-bucket-b

EC2-B â”€â”€(ProjectBucketRole)â”€â”€â–º project-bucket-b
        âŒ training-bucket-a

No cross-access allowed.

ğŸ”¹ STEP-BY-STEP REFERENCE (SECTION-7)
ğŸ”¹ STEP 7.1 â€” Create Two S3 Buckets

Path:
AWS Console â†’ S3 â†’ Create bucket

Bucket-A

Name: training-bucket-a-<unique>

Region: Same as EC2

Block public access: Enabled

Bucket-B

Name: project-bucket-b-<unique>

Region: Same as EC2

Block public access: Enabled

Bucket names must be globally unique

ğŸ”¹ STEP 7.2 â€” Create IAM Role-A (TrainingBucketRole)

Path:
IAM â†’ Roles â†’ Create role

Configuration

Trusted entity: AWS service

Use case: EC2

Role name: TrainingBucketRole

ğŸ”¹ STEP 7.3 â€” Configure Trust Policy (Role-A)

IAM â†’ Roles â†’ TrainingBucketRole â†’ Trust relationships â†’ Edit

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}

Meaning

Allows EC2 instances to assume this role
ğŸ”¹ STEP 7.4 â€” Attach Inline Permission Policy (Role-A)

IAM â†’ Roles â†’ TrainingBucketRole â†’ Permissions â†’ Create inline policy

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::training-bucket-a-*",
        "arn:aws:s3:::training-bucket-a-*/*"
      ]
    },
    {
      "Effect": "Deny",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::project-bucket-b-*",
        "arn:aws:s3:::project-bucket-b-*/*"
      ]
    }
  ]
}

Policy name:

TrainingBucketPolicy

ğŸ”¹ STEP 7.5 â€” Create IAM Role-B (ProjectBucketRole)

Repeat Role creation steps.

Role name:

ProjectBucketRole

ğŸ”¹ STEP 7.6 â€” Configure Trust Policy (Role-B)

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}

ğŸ”¹ STEP 7.7 â€” Attach Inline Permission Policy (Role-B)

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::project-bucket-b-*",
        "arn:aws:s3:::project-bucket-b-*/*"
      ]
    },
    {
      "Effect": "Deny",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::training-bucket-a-*",
        "arn:aws:s3:::training-bucket-a-*/*"
      ]
    }
  ]
}

Policy name:

ProjectBucketPolicy

ğŸ”¹ STEP 7.8 â€” Launch Two EC2 Instances

EC2-A (Training)

Name: ec2-training

AMI: Amazon Linux

IAM Role: TrainingBucketRole

Security Group: SSH allowed

EC2-B (Project)

Name: ec2-project

AMI: Amazon Linux

IAM Role: ProjectBucketRole

Security Group: SSH allowed

ğŸ”¹ STEP 7.9 â€” Verify Role Assumption (MANDATORY)

Inside EC2-A

aws sts get-caller-identity

Must show:

assumed-role/TrainingBucketRole

Inside EC2-B

aws sts get-caller-identity

Must show:

assumed-role/ProjectBucketRole

ğŸ”¹ STEP 7.10 â€” Verify Bucket Access (PROOF)

EC2-A

aws s3 ls s3://training-bucket-a-<unique>    # âœ… Allowed
aws s3 ls s3://project-bucket-b-<unique>     # âŒ AccessDenied

EC2-B

aws s3 ls s3://project-bucket-b-<unique>     # âœ… Allowed
aws s3 ls s3://training-bucket-a-<unique>    # âŒ AccessDenied

âœ… SECTION-7 FINAL CHECKLIST

Requirement Status

Two S3 buckets created âœ…

Two IAM roles created âœ…

Correct trust policies âœ…

Inline allow + explicit deny âœ…

Roles attached to EC2 âœ…

Correct role assumed âœ…

Cross-bucket access denied âœ…
About
No description, website, or topics provided.
Resources
Readme
Activity
Stars
0 stars
Watchers
0 watching
Forks
0 forks
Report repository
Releases
No releases published
Packages
No packages published 
